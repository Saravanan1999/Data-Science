{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX2N0j5hJjg0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "2ee33e5b-6b6f-4c74-d902-a951162b161c"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://raw.githubusercontent.com/Saravanan1999/Data-Science/master/Tensorflow/ngram/poem.txt \\\n",
        "    -O /tmp/poem.txt\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-08 09:51:57--  https://raw.githubusercontent.com/Saravanan1999/Data-Science/master/Tensorflow/poem.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10094 (9.9K) [text/plain]\n",
            "Saving to: ‘/tmp/poem.txt’\n",
            "\n",
            "\r/tmp/poem.txt         0%[                    ]       0  --.-KB/s               \r/tmp/poem.txt       100%[===================>]   9.86K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-04-08 09:51:57 (75.4 MB/s) - ‘/tmp/poem.txt’ saved [10094/10094]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW9b_cecM6vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import numpy as np \n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "poems=open('/tmp/poem.txt').read()\n",
        "text_data=poems.lower().split(\"\\n\")\n",
        "tokenizer.fit_on_texts(text_data)\n",
        "no_of_words=len(tokenizer.word_index) + 1\n",
        "sequence = []\n",
        "for x in text_data:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([x])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tngram_sequence = token_list[:i+1]\n",
        "\t\tsequence.append(ngram_sequence)\n",
        "len1=[]\n",
        "for x in sequence:\n",
        "  len1.append(len(x))\n",
        "longest_seq= max(len1)\n",
        "sequence = np.array(pad_sequences(sequence, maxlen=longest_seq, padding='pre'))\n",
        "train, label = sequence[:,:-1],sequence[:,-1]\n",
        "label = tf.keras.utils.to_categorical(label, num_classes=no_of_words)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDZ_2ftLJx-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5af5851c-a94c-4c2f-faf5-66bcfbcb8460"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  Embedding(no_of_words, 100, input_length=longest_seq-1),\n",
        "  Bidirectional(LSTM(150, return_sequences = True)),\n",
        "  Dropout(0.2),\n",
        "  LSTM(100),\n",
        "  Dense(no_of_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "  Dense(no_of_words, activation='softmax')\n",
        "  \n",
        "])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(train, label, epochs=150, verbose=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 6.4114 - accuracy: 0.0655\n",
            "Epoch 2/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 5.5990 - accuracy: 0.0698\n",
            "Epoch 3/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 5.2533 - accuracy: 0.0667\n",
            "Epoch 4/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 5.0168 - accuracy: 0.0728\n",
            "Epoch 5/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 4.8146 - accuracy: 0.0818\n",
            "Epoch 6/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 4.6336 - accuracy: 0.0938\n",
            "Epoch 7/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 4.4868 - accuracy: 0.0944\n",
            "Epoch 8/150\n",
            "52/52 [==============================] - 3s 64ms/step - loss: 4.3440 - accuracy: 0.1040\n",
            "Epoch 9/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 4.2453 - accuracy: 0.1137\n",
            "Epoch 10/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 4.1411 - accuracy: 0.1191\n",
            "Epoch 11/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 4.0388 - accuracy: 0.1317\n",
            "Epoch 12/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 3.9354 - accuracy: 0.1335\n",
            "Epoch 13/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 3.8508 - accuracy: 0.1497\n",
            "Epoch 14/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 3.7755 - accuracy: 0.1515\n",
            "Epoch 15/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 3.7021 - accuracy: 0.1702\n",
            "Epoch 16/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 3.6345 - accuracy: 0.1738\n",
            "Epoch 17/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 3.5589 - accuracy: 0.1894\n",
            "Epoch 18/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 3.4872 - accuracy: 0.2063\n",
            "Epoch 19/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 3.4045 - accuracy: 0.2453\n",
            "Epoch 20/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 3.3445 - accuracy: 0.2628\n",
            "Epoch 21/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 3.2999 - accuracy: 0.2646\n",
            "Epoch 22/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 3.2405 - accuracy: 0.2886\n",
            "Epoch 23/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 3.1988 - accuracy: 0.2928\n",
            "Epoch 24/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 3.1314 - accuracy: 0.3079\n",
            "Epoch 25/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 3.0531 - accuracy: 0.3337\n",
            "Epoch 26/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 3.0032 - accuracy: 0.3542\n",
            "Epoch 27/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 2.9676 - accuracy: 0.3602\n",
            "Epoch 28/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 2.9010 - accuracy: 0.3860\n",
            "Epoch 29/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 2.8718 - accuracy: 0.3824\n",
            "Epoch 30/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 2.8465 - accuracy: 0.3812\n",
            "Epoch 31/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 2.9957 - accuracy: 0.3584\n",
            "Epoch 32/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 2.8205 - accuracy: 0.4023\n",
            "Epoch 33/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 2.7278 - accuracy: 0.4173\n",
            "Epoch 34/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 2.6542 - accuracy: 0.4402\n",
            "Epoch 35/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 2.6055 - accuracy: 0.4438\n",
            "Epoch 36/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 2.5619 - accuracy: 0.4582\n",
            "Epoch 37/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 2.5347 - accuracy: 0.4636\n",
            "Epoch 38/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 2.5798 - accuracy: 0.4492\n",
            "Epoch 39/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 2.4405 - accuracy: 0.4973\n",
            "Epoch 40/150\n",
            "52/52 [==============================] - 3s 67ms/step - loss: 2.3992 - accuracy: 0.4865\n",
            "Epoch 41/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 2.3398 - accuracy: 0.5213\n",
            "Epoch 42/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 2.2763 - accuracy: 0.5189\n",
            "Epoch 43/150\n",
            "52/52 [==============================] - 3s 67ms/step - loss: 2.2459 - accuracy: 0.5274\n",
            "Epoch 44/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 2.2125 - accuracy: 0.5304\n",
            "Epoch 45/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 2.1510 - accuracy: 0.5400\n",
            "Epoch 46/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 2.1015 - accuracy: 0.5580\n",
            "Epoch 47/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 2.0653 - accuracy: 0.5725\n",
            "Epoch 48/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 2.0194 - accuracy: 0.5725\n",
            "Epoch 49/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 2.0007 - accuracy: 0.5815\n",
            "Epoch 50/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.9609 - accuracy: 0.5833\n",
            "Epoch 51/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.9314 - accuracy: 0.5893\n",
            "Epoch 52/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.9012 - accuracy: 0.5959\n",
            "Epoch 53/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 1.8599 - accuracy: 0.6085\n",
            "Epoch 54/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.8360 - accuracy: 0.6260\n",
            "Epoch 55/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.8010 - accuracy: 0.6272\n",
            "Epoch 56/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.7597 - accuracy: 0.6302\n",
            "Epoch 57/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.7278 - accuracy: 0.6374\n",
            "Epoch 58/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 1.7026 - accuracy: 0.6488\n",
            "Epoch 59/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.8039 - accuracy: 0.6284\n",
            "Epoch 60/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 1.8103 - accuracy: 0.6170\n",
            "Epoch 61/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 1.7096 - accuracy: 0.6518\n",
            "Epoch 62/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.6290 - accuracy: 0.6681\n",
            "Epoch 63/150\n",
            "52/52 [==============================] - 3s 64ms/step - loss: 1.5877 - accuracy: 0.6831\n",
            "Epoch 64/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.5749 - accuracy: 0.6723\n",
            "Epoch 65/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.5565 - accuracy: 0.6831\n",
            "Epoch 66/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.5065 - accuracy: 0.6939\n",
            "Epoch 67/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.4638 - accuracy: 0.7048\n",
            "Epoch 68/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.4405 - accuracy: 0.7084\n",
            "Epoch 69/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.4232 - accuracy: 0.7120\n",
            "Epoch 70/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.3920 - accuracy: 0.7300\n",
            "Epoch 71/150\n",
            "52/52 [==============================] - 3s 64ms/step - loss: 1.3700 - accuracy: 0.7360\n",
            "Epoch 72/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.3412 - accuracy: 0.7348\n",
            "Epoch 73/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 1.3237 - accuracy: 0.7294\n",
            "Epoch 74/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.2945 - accuracy: 0.7444\n",
            "Epoch 75/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 1.2625 - accuracy: 0.7565\n",
            "Epoch 76/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 1.2464 - accuracy: 0.7589\n",
            "Epoch 77/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.2322 - accuracy: 0.7613\n",
            "Epoch 78/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.2161 - accuracy: 0.7655\n",
            "Epoch 79/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 1.2036 - accuracy: 0.7697\n",
            "Epoch 80/150\n",
            "52/52 [==============================] - 3s 67ms/step - loss: 1.1611 - accuracy: 0.7775\n",
            "Epoch 81/150\n",
            "52/52 [==============================] - 3s 67ms/step - loss: 1.1472 - accuracy: 0.7847\n",
            "Epoch 82/150\n",
            "52/52 [==============================] - 3s 67ms/step - loss: 1.1466 - accuracy: 0.7805\n",
            "Epoch 83/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.1459 - accuracy: 0.7883\n",
            "Epoch 84/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.1353 - accuracy: 0.7823\n",
            "Epoch 85/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 1.1161 - accuracy: 0.7974\n",
            "Epoch 86/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.1103 - accuracy: 0.7865\n",
            "Epoch 87/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.0753 - accuracy: 0.8040\n",
            "Epoch 88/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 1.0826 - accuracy: 0.7974\n",
            "Epoch 89/150\n",
            "52/52 [==============================] - 3s 64ms/step - loss: 1.0515 - accuracy: 0.8064\n",
            "Epoch 90/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 1.0225 - accuracy: 0.8094\n",
            "Epoch 91/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 1.0098 - accuracy: 0.8214\n",
            "Epoch 92/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.9848 - accuracy: 0.8196\n",
            "Epoch 93/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.9821 - accuracy: 0.8232\n",
            "Epoch 94/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.9696 - accuracy: 0.8256\n",
            "Epoch 95/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.9502 - accuracy: 0.8214\n",
            "Epoch 96/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.9421 - accuracy: 0.8316\n",
            "Epoch 97/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.9450 - accuracy: 0.8274\n",
            "Epoch 98/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.9186 - accuracy: 0.8358\n",
            "Epoch 99/150\n",
            "52/52 [==============================] - 3s 67ms/step - loss: 0.9099 - accuracy: 0.8304\n",
            "Epoch 100/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.9054 - accuracy: 0.8322\n",
            "Epoch 101/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.8892 - accuracy: 0.8364\n",
            "Epoch 102/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.8766 - accuracy: 0.8400\n",
            "Epoch 103/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.8712 - accuracy: 0.8352\n",
            "Epoch 104/150\n",
            "52/52 [==============================] - 3s 64ms/step - loss: 0.8613 - accuracy: 0.8485\n",
            "Epoch 105/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.8525 - accuracy: 0.8413\n",
            "Epoch 106/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.8438 - accuracy: 0.8479\n",
            "Epoch 107/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.8818 - accuracy: 0.8370\n",
            "Epoch 108/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.8783 - accuracy: 0.8352\n",
            "Epoch 109/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.8682 - accuracy: 0.8382\n",
            "Epoch 110/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.8198 - accuracy: 0.8581\n",
            "Epoch 111/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.8225 - accuracy: 0.8491\n",
            "Epoch 112/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.8114 - accuracy: 0.8503\n",
            "Epoch 113/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.8067 - accuracy: 0.8479\n",
            "Epoch 114/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.8024 - accuracy: 0.8467\n",
            "Epoch 115/150\n",
            "52/52 [==============================] - 3s 67ms/step - loss: 0.7732 - accuracy: 0.8527\n",
            "Epoch 116/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.7638 - accuracy: 0.8617\n",
            "Epoch 117/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.7601 - accuracy: 0.8563\n",
            "Epoch 118/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.7603 - accuracy: 0.8605\n",
            "Epoch 119/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.7499 - accuracy: 0.8593\n",
            "Epoch 120/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.7438 - accuracy: 0.8581\n",
            "Epoch 121/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.7297 - accuracy: 0.8707\n",
            "Epoch 122/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.7313 - accuracy: 0.8659\n",
            "Epoch 123/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.7247 - accuracy: 0.8617\n",
            "Epoch 124/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.7129 - accuracy: 0.8677\n",
            "Epoch 125/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.7177 - accuracy: 0.8617\n",
            "Epoch 126/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.6977 - accuracy: 0.8665\n",
            "Epoch 127/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.7082 - accuracy: 0.8659\n",
            "Epoch 128/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6942 - accuracy: 0.8713\n",
            "Epoch 129/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.6947 - accuracy: 0.8659\n",
            "Epoch 130/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6840 - accuracy: 0.8659\n",
            "Epoch 131/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.6747 - accuracy: 0.8743\n",
            "Epoch 132/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6862 - accuracy: 0.8719\n",
            "Epoch 133/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6808 - accuracy: 0.8665\n",
            "Epoch 134/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6629 - accuracy: 0.8701\n",
            "Epoch 135/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6555 - accuracy: 0.8683\n",
            "Epoch 136/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6541 - accuracy: 0.8767\n",
            "Epoch 137/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6524 - accuracy: 0.8725\n",
            "Epoch 138/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6428 - accuracy: 0.8821\n",
            "Epoch 139/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6640 - accuracy: 0.8689\n",
            "Epoch 140/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6510 - accuracy: 0.8671\n",
            "Epoch 141/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.7186 - accuracy: 0.8521\n",
            "Epoch 142/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6852 - accuracy: 0.8647\n",
            "Epoch 143/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6734 - accuracy: 0.8647\n",
            "Epoch 144/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6734 - accuracy: 0.8647\n",
            "Epoch 145/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6461 - accuracy: 0.8677\n",
            "Epoch 146/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6157 - accuracy: 0.8755\n",
            "Epoch 147/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.6097 - accuracy: 0.8809\n",
            "Epoch 148/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6180 - accuracy: 0.8761\n",
            "Epoch 149/150\n",
            "52/52 [==============================] - 3s 65ms/step - loss: 0.6199 - accuracy: 0.8719\n",
            "Epoch 150/150\n",
            "52/52 [==============================] - 3s 66ms/step - loss: 0.6091 - accuracy: 0.8773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuJLC_T9PksL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "3dcf2f5b-dac3-4453-e76f-d8deb13932e4"
      },
      "source": [
        "start = str(input()) #Enter a starting line or phrase\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken = tokenizer.texts_to_sequences([start])[0]\n",
        "\ttoken = pad_sequences([token], maxlen=longest_seq-1, padding='pre')\n",
        "\tmodel_predict = model.predict_classes(token, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == model_predict:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tstart = start +\" \" + output_word\n",
        "print(start)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cow\n",
            "cow they were neither up nor down will come baby cradle and all that half was up the hill don’t sing of the hill don’t sing sick quick quick quick quick quick quick quick quick quick quick quick a fall a all you were a your you the black sheep a turn sick sick sick sick sick sick sick sick sick sick a sheep sheep cradle a head your you i cradle a your it bit my bark blue chop chop chop chop chop chop chop chop chop chop chop chop chop cradle on were jump round i round i round\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}